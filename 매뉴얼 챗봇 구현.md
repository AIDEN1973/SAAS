ğŸ“š ë§¤ë‰´ì–¼ ì±—ë´‡ í†µí•© ë°©ë²• ë¶„ì„
ğŸ“Š í˜„ì¬ ìƒí™©
ë§¤ë‰´ì–¼ ë°ì´í„° ê·œëª¨:

ì´ 14ê°œ ë§¤ë‰´ì–¼ íŒŒì¼ (dashboard, students, attendance, etc.)
ì•½ 3,874ì¤„ (TypeScript)
ì¶”ì • í† í° ìˆ˜: ~25,000-30,000 tokens (í•œê¸€ í¬í•¨)
ê° ë§¤ë‰´ì–¼ í‰ê·  í¬ê¸°: ~2,000-2,500 tokens
í˜„ì¬ AI ì—ì´ì „íŠ¸ êµ¬ì¡°:

OpenAI gpt-4o-mini ì‚¬ìš©
ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°: 128K tokens
í˜„ì¬ ì‚¬ìš©ëŸ‰: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ëŒ€í™” 6í„´ + ë„êµ¬ ì •ì˜ = ~3,000-5,000 tokens
ì—¬ìœ  ê³µê°„: ~120,000 tokens
ë¬¸ì œì :
âœ… í† í° ì—¬ìœ ëŠ” ì¶©ë¶„í•¨ (ì „ì²´ ë§¤ë‰´ì–¼ì„ ë„£ì–´ë„ 25-30% ì‚¬ìš©)
âŒ ë§¤ ìš”ì²­ë§ˆë‹¤ ì „ì²´ ë§¤ë‰´ì–¼ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ë©´:

ì…ë ¥ í† í° ë¹„ìš© í­ì¦ ($0.15/1M tokens Ã— 30K = $0.0045/ìš”ì²­)
ì‘ë‹µ ì†ë„ ì €í•˜ (ì…ë ¥ ì²˜ë¦¬ ì‹œê°„ ì¦ê°€)
ë¶ˆí•„ìš”í•œ ì •ë³´ê¹Œì§€ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
ğŸ¯ í•´ê²° ë°©ë²• 5ê°€ì§€
ë°©ë²• 1: RAG (Retrieval Augmented Generation) â­â­â­â­â­ ìµœê³  ê¶Œì¥
ê°œë…:
ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë§¤ë‰´ì–¼ ì„¹ì…˜ë§Œ ê²€ìƒ‰í•´ì„œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€

êµ¬í˜„ ë°©ì‹:

A. Vector Database + Semantic Search (ìµœì )

// 1ë‹¨ê³„: ë§¤ë‰´ì–¼ ì„ë² ë”© ìƒì„± (ë¹Œë“œ/ë°°í¬ ì‹œ 1íšŒ)
async function embedManuals() {
  const chunks = [];

  for (const manual of allManualPages) {
    for (const section of manual.sections) {
      // ì„¹ì…˜ë³„ë¡œ ì²­í¬ ìƒì„±
      const chunk = {
        id: `${manual.id}-${section.id}`,
        manual_id: manual.id,
        manual_title: manual.title,
        section_title: section.title,
        content: extractSectionContent(section), // í…ìŠ¤íŠ¸ ì¶”ì¶œ
        metadata: {
          type: section.type,
          manual_description: manual.description,
        }
      };

      // OpenAI Embeddings API í˜¸ì¶œ
      const embedding = await openai.embeddings.create({
        model: "text-embedding-3-small", // $0.02/1M tokens
        input: chunk.content
      });

      chunks.push({
        ...chunk,
        embedding: embedding.data[0].embedding // 1536 dimensions
      });
    }
  }

  // Supabase pgvectorì— ì €ì¥
  await supabase.from('manual_embeddings').insert(chunks);
}

// 2ë‹¨ê³„: ì‚¬ìš©ì ì§ˆë¬¸ ì‹œ ê´€ë ¨ ì„¹ì…˜ ê²€ìƒ‰
async function searchRelevantManuals(userQuery: string, limit = 3) {
  // ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
  const queryEmbedding = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: userQuery
  });

  // ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê°€ì¥ ê´€ë ¨ìˆëŠ” ì„¹ì…˜ ê²€ìƒ‰
  const results = await supabase.rpc('match_manual_sections', {
    query_embedding: queryEmbedding.data[0].embedding,
    match_threshold: 0.7, // ìœ ì‚¬ë„ ì„ê³„ê°’
    match_count: limit
  });

  return results.data; // Top 3 ê´€ë ¨ ì„¹ì…˜
}

// 3ë‹¨ê³„: ì±—ë´‡ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…
async function handleChatQuery(userMessage: string) {
  const relevantSections = await searchRelevantManuals(userMessage);

  const systemPrompt = `
ë‹¹ì‹ ì€ í•™ì› ê´€ë¦¬ ì‹œìŠ¤í…œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë§¤ë‰´ì–¼ ë‚´ìš©ì…ë‹ˆë‹¤:
${relevantSections.map(s => `
## ${s.manual_title} - ${s.section_title}
${s.content}
`).join('\n')}

ìœ„ ë§¤ë‰´ì–¼ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
  `;

  // OpenAI API í˜¸ì¶œ
  const response = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
      { role: "system", content: systemPrompt },
      { role: "user", content: userMessage }
    ]
  });

  return response.choices[0].message.content;
}
PostgreSQL (Supabase pgvector) ìŠ¤í‚¤ë§ˆ:


-- pgvector í™•ì¥ í™œì„±í™”
CREATE EXTENSION IF NOT EXISTS vector;

-- ë§¤ë‰´ì–¼ ì„ë² ë”© í…Œì´ë¸”
CREATE TABLE manual_embeddings (
  id TEXT PRIMARY KEY,
  manual_id TEXT NOT NULL,
  manual_title TEXT NOT NULL,
  section_id TEXT NOT NULL,
  section_title TEXT NOT NULL,
  section_type TEXT NOT NULL,
  content TEXT NOT NULL,
  metadata JSONB,
  embedding vector(1536), -- OpenAI embedding í¬ê¸°
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ì¸ë±ìŠ¤ (HNSW - ë¹ ë¥¸ ê·¼ì‚¬ ê²€ìƒ‰)
CREATE INDEX ON manual_embeddings
USING hnsw (embedding vector_cosine_ops);

-- ê²€ìƒ‰ í•¨ìˆ˜
CREATE FUNCTION match_manual_sections(
  query_embedding vector(1536),
  match_threshold float,
  match_count int
)
RETURNS TABLE (
  id text,
  manual_title text,
  section_title text,
  content text,
  similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    manual_embeddings.id,
    manual_embeddings.manual_title,
    manual_embeddings.section_title,
    manual_embeddings.content,
    1 - (manual_embeddings.embedding <=> query_embedding) as similarity
  FROM manual_embeddings
  WHERE 1 - (manual_embeddings.embedding <=> query_embedding) > match_threshold
  ORDER BY manual_embeddings.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
ì¥ì :

âœ… í† í° ì ˆì•½: ì „ì²´ 30K â†’ ê´€ë ¨ 3ê°œ ì„¹ì…˜ ~2-3K tokens (90% ì ˆê°)
âœ… ì •í™•ë„: ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ìœ¼ë¡œ ì •í™•í•œ ì„¹ì…˜ ì°¾ê¸°
âœ… ì†ë„: ì„ë² ë”© ê²€ìƒ‰ì€ ì„œë¸Œë°€ë¦¬ì´ˆê¸‰
âœ… í™•ì¥ì„±: ë§¤ë‰´ì–¼ ì¶”ê°€/ìˆ˜ì • ì‹œ í•´ë‹¹ ì„¹ì…˜ë§Œ ì¬ì„ë² ë”©
ë‹¨ì :

âŒ ì´ˆê¸° êµ¬í˜„ ë¹„ìš© (pgvector ì„¤ì •, ì„ë² ë”© íŒŒì´í”„ë¼ì¸)
âŒ ì„ë² ë”© API ë¹„ìš© (~$0.02/1M tokens, ì´ˆê¸° 1íšŒ + ë§¤ë‰´ì–¼ ìˆ˜ì • ì‹œ)
ë¹„ìš© ë¶„ì„:


ì´ˆê¸° ì„ë² ë”© ë¹„ìš©:
- ë§¤ë‰´ì–¼ ì´ í† í°: 30,000
- ì„ë² ë”© ë¹„ìš©: 30,000 Ã— $0.02 / 1,000,000 = $0.0006 (1íšŒ)

ì‹¤í–‰ ì‹œ ë¹„ìš© (ì‚¬ìš©ì ì§ˆë¬¸ë‹¹):
- ì§ˆë¬¸ ì„ë² ë”©: ~50 tokens Ã— $0.02 / 1M = $0.000001
- ê´€ë ¨ ì„¹ì…˜ 3ê°œ: ~3,000 tokens (ì…ë ¥)
- vs ì „ì²´ ë§¤ë‰´ì–¼: ~30,000 tokens (ì…ë ¥)
- ì ˆì•½: 27,000 tokens Ã— $0.15 / 1M = $0.00405/ì§ˆë¬¸

â†’ 100 ì§ˆë¬¸ ê¸°ì¤€: $0.405 ì ˆì•½
B. Keyword-Based ê²€ìƒ‰ (ê°„ë‹¨í•œ RAG)
Vector DB ì—†ì´ PostgreSQL FTSë¡œ êµ¬í˜„:


// ë§¤ë‰´ì–¼ ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
async function searchManualsByKeywords(query: string) {
  const tsquery = query.split(' ').map(w => `${w}:*`).join(' & ');

  const results = await supabase.rpc('search_manuals', {
    search_query: tsquery,
    limit_count: 3
  });

  return results.data;
}
ì¥ì :

âœ… êµ¬í˜„ ê°„ë‹¨ (ì´ë¯¸ FTS ì¸í”„ë¼ ì¡´ì¬)
âœ… ë¹„ìš© ì—†ìŒ (ì„ë² ë”© API ë¶ˆí•„ìš”)
ë‹¨ì :

âŒ ì •í™•ë„ ë‚®ìŒ (í‚¤ì›Œë“œë§Œ ë§¤ì¹­, ì˜ë¯¸ ì´í•´ X)
âŒ í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ ë¶€ì¬ (simple ì‚¬ì „)
ë°©ë²• 2: í”„ë¡¬í”„íŠ¸ ì••ì¶• (Prompt Compression) â­â­â­
ì „ì²´ ë§¤ë‰´ì–¼ì„ ì••ì¶•í•´ì„œ í† í° ì ˆê°:


// LLMLingua ê°™ì€ ì••ì¶• ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
import { compressPrompt } from 'llm-compressor';

const compressedManuals = compressPrompt(allManualsText, {
  compressionRatio: 0.5, // 50% ì••ì¶•
  preserveKeywords: ['ê²€ìƒ‰', 'ë“±ë¡', 'ìˆ˜ì •', 'ì‚­ì œ'], // í•µì‹¬ í‚¤ì›Œë“œ ë³´ì¡´
});

// 30K tokens â†’ 15K tokens
ì¥ì :

âœ… RAG ì—†ì´ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
âœ… êµ¬í˜„ ê°„ë‹¨
ë‹¨ì :

âŒ ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„±
âŒ ì—¬ì „íˆ ë§¤ ìš”ì²­ë§ˆë‹¤ 15K í† í° ì‚¬ìš©
âŒ ì••ì¶• ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
ë°©ë²• 3: ê³„ì¸µì  RAG (Hierarchical RAG) â­â­â­â­
2ë‹¨ê³„ ê²€ìƒ‰:


// 1ë‹¨ê³„: ë§¤ë‰´ì–¼ ë ˆë²¨ ê²€ìƒ‰
async function findRelevantManual(query: string) {
  // "ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš©ë²•" â†’ 'search' ë§¤ë‰´ì–¼ ì„ íƒ
  const manualEmbeddings = await searchManualDescriptions(query);
  return manualEmbeddings[0].manual_id; // 'search'
}

// 2ë‹¨ê³„: ì„ íƒëœ ë§¤ë‰´ì–¼ ë‚´ ì„¹ì…˜ ê²€ìƒ‰
async function findRelevantSections(manualId: string, query: string) {
  const sections = await searchSectionsInManual(manualId, query);
  return sections.slice(0, 2); // Top 2 ì„¹ì…˜
}

// ìµœì¢…: 1ê°œ ë§¤ë‰´ì–¼ Ã— 2ê°œ ì„¹ì…˜ = ~1-2K tokensë§Œ ì‚¬ìš©
ì¥ì :

âœ… ë” ì •í™•í•œ ê²€ìƒ‰ (ê³„ì¸µ êµ¬ì¡° í™œìš©)
âœ… í† í° ì‚¬ìš© ìµœì†Œí™”
ë‹¨ì :

âŒ êµ¬í˜„ ë³µì¡ë„ ì¦ê°€
âŒ 2ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ ë ˆì´í„´ì‹œ ì¦ê°€
ë°©ë²• 4: ìºì‹± (Prompt Caching) â­â­â­â­â­
OpenAI/Anthropicì˜ í”„ë¡¬í”„íŠ¸ ìºì‹± ê¸°ëŠ¥ í™œìš©:


// Anthropic Claude API (Prompt Caching ì§€ì›)
const response = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  system: [
    {
      type: "text",
      text: "ë‹¹ì‹ ì€ í•™ì› ê´€ë¦¬ ì‹œìŠ¤í…œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",
    },
    {
      type: "text",
      text: allManualsText, // ì „ì²´ ë§¤ë‰´ì–¼ 30K tokens
      cache_control: { type: "ephemeral" } // ğŸ”‘ ìºì‹±!
    }
  ],
  messages: [...]
});
ì‘ë™ ì›ë¦¬:

ì²« ìš”ì²­: ì „ì²´ 30K tokens ì²˜ë¦¬ (ì •ê°€)
5ë¶„ ë‚´ í›„ì† ìš”ì²­: ìºì‹œëœ 30K tokensëŠ” 90% í• ì¸ (Anthropic ê¸°ì¤€)
ìºì‹œ TTL: 5ë¶„
ë¹„ìš© ë¶„ì„ (Anthropic Claude):


ì¼ë°˜ ìš”ì²­:
- ì…ë ¥: 30,000 tokens Ã— $3.00 / 1M = $0.09

ìºì‹± í›„ (5ë¶„ ë‚´):
- ìºì‹œ ì½ê¸°: 30,000 tokens Ã— $0.30 / 1M = $0.009 (90% í• ì¸)
- ì ˆì•½: $0.081/ìš”ì²­

â†’ ì‚¬ìš©ìê°€ 5ë¶„ ë‚´ 10ê°œ ì§ˆë¬¸: $0.81 ì ˆì•½
ì¥ì :

âœ… ì „ì²´ ë§¤ë‰´ì–¼ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
âœ… RAG êµ¬í˜„ ë¶ˆí•„ìš”
âœ… 90% ë¹„ìš© ì ˆê° (ë°˜ë³µ ìš”ì²­ ì‹œ)
ë‹¨ì :

âŒ OpenAIëŠ” ì•„ì§ í”„ë¡¬í”„íŠ¸ ìºì‹± ë¯¸ì§€ì› (Anthropicë§Œ ì§€ì›)
âŒ 5ë¶„ TTL ì´í›„ì—” ë‹¤ì‹œ ì „ì²´ ë¹„ìš©
âŒ í˜„ì¬ gpt-4o-mini â†’ claude-3-5-sonnet ì „í™˜ í•„ìš”
ë°©ë²• 5: í•˜ì´ë¸Œë¦¬ë“œ: RAG + ìºì‹± â­â­â­â­â­ ìµœì¢… ê¶Œì¥

// 1. ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ê´€ë ¨ ë§¤ë‰´ì–¼ 3ê°œ ê²€ìƒ‰ (RAG)
const relevantSections = await searchRelevantManuals(userQuery, 3);

// 2. ê²€ìƒ‰ëœ ë§¤ë‰´ì–¼ì„ ìºì‹± (Anthropic Prompt Caching)
const response = await anthropic.messages.create({
  model: "claude-3-5-sonnet-20241022",
  system: [
    {
      type: "text",
      text: baseSystemPrompt,
    },
    {
      type: "text",
      text: formatManualSections(relevantSections), // 3K tokens
      cache_control: { type: "ephemeral" } // ìºì‹±
    }
  ],
  messages: conversationHistory
});
ì¥ì :

âœ… ì •í™•ë„ (RAG)
âœ… ë¹„ìš© ì ˆê° (ìºì‹±)
âœ… ì†ë„ (ê´€ë ¨ ì„¹ì…˜ë§Œ)
ğŸ“Š ë°©ë²•ë³„ ë¹„êµí‘œ
ë°©ë²•	í† í° ì‚¬ìš©	ë¹„ìš©/ìš”ì²­	ì •í™•ë„	êµ¬í˜„ ë‚œì´ë„	ì†ë„
RAG (Vector)	3K	$0.0005	â­â­â­â­â­	ğŸ”¨ğŸ”¨ğŸ”¨	âš¡âš¡âš¡âš¡
RAG (Keyword)	3K	$0.0005	â­â­â­	ğŸ”¨	âš¡âš¡âš¡âš¡âš¡
í”„ë¡¬í”„íŠ¸ ì••ì¶•	15K	$0.0023	â­â­â­â­	ğŸ”¨ğŸ”¨	âš¡âš¡âš¡
ê³„ì¸µ RAG	2K	$0.0003	â­â­â­â­â­	ğŸ”¨ğŸ”¨ğŸ”¨ğŸ”¨	âš¡âš¡âš¡
í”„ë¡¬í”„íŠ¸ ìºì‹±	30K	$0.009 (ìºì‹œ)	â­â­â­â­â­	ğŸ”¨	âš¡âš¡âš¡âš¡
í•˜ì´ë¸Œë¦¬ë“œ	3K	$0.0005 (ìºì‹œ)	â­â­â­â­â­	ğŸ”¨ğŸ”¨ğŸ”¨	âš¡âš¡âš¡âš¡â­
ì „ì²´ ì£¼ì…	30K	$0.0045	â­â­â­â­â­	ğŸ”¨	âš¡âš¡
ğŸ† ìµœì¢… ê¶Œì¥ ì†”ë£¨ì…˜
ë‹¨ê¸° (ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥): Keyword-Based RAG

// 1. PostgreSQL FTSë¡œ ë§¤ë‰´ì–¼ ê²€ìƒ‰ (ì´ë¯¸ êµ¬í˜„ëœ ì¸í”„ë¼ ì¬ì‚¬ìš©)
// 2. ê´€ë ¨ 3ê°œ ì„¹ì…˜ë§Œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
// 3. ë¹„ìš© 90% ì ˆê°, êµ¬í˜„ 1-2ì¼
ì¤‘ê¸° (ìµœì í™”): Vector RAG (pgvector)

// 1. Supabase pgvector í™œì„±í™”
// 2. ë§¤ë‰´ì–¼ ì„ë² ë”© ìƒì„± (ë¹Œë“œ ì‹œ)
// 3. ì˜ë¯¸ë¡ ì  ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
// 4. êµ¬í˜„ 3-5ì¼
ì¥ê¸° (ì´ìƒì ): í•˜ì´ë¸Œë¦¬ë“œ (Vector RAG + Prompt Caching)

// 1. Vector RAGë¡œ ê´€ë ¨ ì„¹ì…˜ ê²€ìƒ‰
// 2. Anthropic Claude + Prompt Caching
// 3. ìµœê³  ì •í™•ë„ + ìµœì € ë¹„ìš©
// 4. êµ¬í˜„ 5-7ì¼ + Claude ì „í™˜ ê²°ì • í•„ìš”
ğŸ’» êµ¬í˜„ ì˜ˆì‹œ (Keyword RAG - ê°€ì¥ ë¹ ë¥¸ ì‹œì‘)

// Step 1: ë§¤ë‰´ì–¼ ê²€ìƒ‰ í…Œì´ë¸” ìƒì„±
CREATE TABLE manual_search_index (
  id TEXT PRIMARY KEY,
  manual_id TEXT,
  manual_title TEXT,
  section_id TEXT,
  section_title TEXT,
  section_type TEXT,
  content TEXT,
  search_vector tsvector GENERATED ALWAYS AS (
    setweight(to_tsvector('simple', coalesce(manual_title, '')), 'A') ||
    setweight(to_tsvector('simple', coalesce(section_title, '')), 'A') ||
    setweight(to_tsvector('simple', coalesce(content, '')), 'B')
  ) STORED
);

CREATE INDEX idx_manual_search_fts ON manual_search_index USING GIN(search_vector);

// Step 2: ë§¤ë‰´ì–¼ ë°ì´í„° ì‚½ì… (ë¹Œë“œ ì‹œ 1íšŒ)
INSERT INTO manual_search_index (id, manual_id, manual_title, ...)
VALUES (...); -- ëª¨ë“  ì„¹ì…˜ ì‚½ì…

// Step 3: Agent ë„êµ¬ ì¶”ê°€
{
  name: "search_manual",
  description: "ì‚¬ìš©ìê°€ ì‹œìŠ¤í…œ ì‚¬ìš©ë²•ì„ ë¬¼ì–´ë³¼ ë•Œ ê´€ë ¨ ë§¤ë‰´ì–¼ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
  parameters: {
    type: "object",
    properties: {
      query: { type: "string", description: "ê²€ìƒ‰ì–´" }
    },
    required: ["query"]
  }
}

// Step 4: ë„êµ¬ ì‹¤í–‰ í•¨ìˆ˜
async function searchManualTool(query: string) {
  const results = await supabase.rpc('search_manual_sections', {
    search_query: query,
    limit: 3
  });

  return results.data.map(r => ({
    title: `${r.manual_title} - ${r.section_title}`,
    content: r.content
  }));
}

// Step 5: AIê°€ ìë™ìœ¼ë¡œ ë§¤ë‰´ì–¼ ê²€ìƒ‰ + ë‹µë³€
// User: "ê²€ìƒ‰ ê¸°ëŠ¥ ì–´ë–»ê²Œ ì¨?"
// AI: [search_manual ë„êµ¬ í˜¸ì¶œ â†’ ê²€ìƒ‰ ë§¤ë‰´ì–¼ ì„¹ì…˜ ê°€ì ¸ì˜´ â†’ ë‹µë³€ ìƒì„±]
ğŸ¯ ê²°ë¡ 
ë§¤ë‰´ì–¼ì„ ì±—ë´‡ì— í†µí•©í•˜ë ¤ë©´:

âœ… ì¦‰ì‹œ ì‹œì‘: Keyword RAG (FTS ê²€ìƒ‰)

ë¹„ìš©: 90% ì ˆê°
êµ¬í˜„: 1-2ì¼
ì •í™•ë„: ì¤‘ê°„
âœ… ìµœì  ì†”ë£¨ì…˜: Vector RAG (pgvector + OpenAI Embeddings)

ë¹„ìš©: 90% ì ˆê° + ì •í™•ë„ ìµœëŒ€í™”
êµ¬í˜„: 3-5ì¼
ì¥ê¸°ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ìŒ
âš ï¸ ê³ ë ¤ ì‚¬í•­: Anthropic Claude + Prompt Caching

ê¸°ì¡´ OpenAI â†’ Claude ì „í™˜ í•„ìš”
ì„¸ì…˜ ë‚´ ë°˜ë³µ ì§ˆë¬¸ ì‹œ ìœ ë¦¬
RAGì™€ ê²°í•©í•˜ë©´ ìµœê³  íš¨ìœ¨
ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:
âŒ ë§¤ ìš”ì²­ë§ˆë‹¤ ì „ì²´ 30K í† í° ë§¤ë‰´ì–¼ ì£¼ì…

ë¹„ìš© 10ë°° ì¦ê°€
ì‘ë‹µ ì†ë„ ì €í•˜
ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì˜¤ì—¼
